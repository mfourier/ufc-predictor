<h1 align="center">
  ğŸ¥‹ UFC Fight Predictor Model
  <img src="img/ufc_logo.png" width="70" style="vertical-align: middle; margin-left: 10px;" />
</h1>

---

## ğŸ¯ Objective

This project aims to build a robust **binary classification model** to predict the winner of a UFC fight. The model estimates whether **Fighter Red** or **Fighter Blue** is more likely to win based on differences in physical attributes, fighting styles, and recent performances.

By transforming fighter-level data into **relative feature vectors**, the model learns from historical outcomes and generalizes effectively to future matchups.

---

## ğŸ“Š Dataset Description

The dataset includes detailed information on historical UFC fights. Each row represents a single bout with features combining:

- ğŸ§â€â™‚ï¸ **Numerical attributes** (e.g., height, reach, age)
- ğŸ¯ **Categorical encodings** (fighting style: ortodox, southpaw, switch, fight stance: open, closed, weight classes.)
- ğŸ“ˆ **Performance indicators** (e.g., striking landed per minute, average takedown attempts)

All features are encoded *relatively*:
$$x = fighter_{blue} - fighter_{red}$$

### Key Feature Groups

- **Fighter Attributes**: Height, reach, weight class, stance, age.  
- **Style & Stance**: One-hot encoded during preprocessing.  
- **Performance Metrics**: Strikes per minute, accuracy, takedown attempts.  
- **Recent Form**: Win/loss streaks, odds. 

### ğŸ¯ Target Variable:
- **0** â†’ Fighter Red wins  
- **1** â†’ Fighter Blue wins  

---

## ğŸ› ï¸ Modeling Approach

The modeling pipeline is organized into three interconnected stages:

1. **Feature Engineering**
   - Fighter data is transformed into **relative differences** between Blue and Red fighters, covering height, reach, age, striking stats, grappling stats, and win streaks.
   - Categorical variables (e.g., stance, fighting style, weight class) are one-hot encoded, using binary encoding for two-class categories and full dummies for multiclass features.
   - Numerical features are standardized using scalers fitted exclusively on the training set, ensuring no data leakage.
   - Additional features are engineered to capture recent activity, such as experience-per-age difference (total rounds fought divided by age), win-by-decision rate difference, and win-by-finish rate difference.
   - Feature selection is guided by correlation analysis, aiming to minimize inter-feature correlation while preserving predictive signal.
   - A synthetic random noise feature (`Random_Noise`) is introduced as a baseline for feature importance: different combinations were explored until the random column gained prominence, guiding the final selection. This iterative approach led to a feature set that maximizes predictive power without overfitting.

2. **Model Training**
   - A diverse suite of machine learning models is trained, combining **classical algorithms**, **boosted ensemble methods**, and **deep learning architectures**.
   - The task is framed as a binary classification problem, with a baseline distribution of approximately 58% red corner wins, reflecting historical outcome imbalance.
   - Hyperparameter tuning is conducted in the notebook `04-training.ipynb`, where detailed parameter grids are defined for each model using `GridSearchCV`. This systematic exploration includes models such as XGBoost, SVM, Random Forest, AdaBoost, and Neural Networks, optimizing performance across algorithmic families.

3. **Evaluation**
   - Model evaluation leverages a comprehensive set of metrics, computed via the modular `metrics.py` implementation:
     - **Accuracy** (0â€“1, higher is better): Overall proportion of correct predictions.
     - **Precision** (0â€“1, higher is better): Fraction of positive predictions that are actually correct.
     - **Recall** (0â€“1, higher is better): Fraction of true positives correctly identified.
     - **F1 Score** (0â€“1, higher is better): Harmonic mean of precision and recall.
     - **ROC-AUC** (0.5â€“1, higher is better): Probability the model ranks a random positive higher than a random negative.
     - **Brier Score** (0â€“1, lower is better): Mean squared error between predicted probabilities and actual outcomes, reflecting calibration.
   - Confusion matrices visualize classification performance across true/false positives and negatives.
   - The framework supports automated multi-model comparison, identifying the top-performing model per metric, enabling robust benchmarking.

---

## ğŸ¤– Models Implemented

The following classifiers have been integrated and carefully tuned, all coordinated through the modular `model_factory.py` pipeline, enabling systematic benchmarking and performance optimization:

- ğŸ”¹ **Classical Models**
  - âœ… **K-Nearest Neighbors (KNN)**: Classifies based on proximity to neighboring points in feature space.
  - âœ… **Support Vector Machine (SVM)**: Effective in high-dimensional, binary classification tasks.
  - âœ… **Logistic Regression**: Linear classifier with probabilistic outputs.
  - âœ… **Naive Bayes**: Probabilistic model suited for high-dimensional feature spaces.
  - âœ… **Quadratic Discriminant Analysis (QDA)**: Assumes Gaussian class-conditional distributions.

- ğŸ”¹ **Ensemble Methods**
  - âœ… **Random Forest**: Bagging ensemble of decision trees, providing robustness and low variance.
  - âœ… **Extra Trees**: Randomized ensemble variant of Random Forest, enhancing variance reduction.

- ğŸ”¹ **Boosted Ensemble Models**
  - âœ… **AdaBoost**: Sequentially combines weak learners to focus on difficult samples.
  - âœ… **Gradient Boosting**: Iteratively builds additive models to minimize prediction error.
  - âœ… **XGBoost**: Highly optimized gradient boosting with regularization, parallelism, and advanced hyperparameter tuning.

- ğŸ”¹ **Deep Learning**
  - âœ… **Neural Networks (MLP)**: Multi-layer perceptron capable of capturing complex, non-linear relationships.

---

## ğŸ§ª Project Structure

```bash
ufc-predictor/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Original fight data
â”‚   â”œâ”€â”€ processed/                   # Cleaned and transformed datasets
â”‚   â””â”€â”€ results/                     # Evaluation logs, metrics, model reports
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-etl.ipynb                 # Data extraction and cleaning
â”‚   â”œâ”€â”€ 02-eda.ipynb                 # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 03-feature_engineering.ipynb # Feature engineering using UFCData
â”‚   â”œâ”€â”€ 04-training.ipynb            # Model training using training set
â”‚   â””â”€â”€ 05-model_experiments.ipynb   # Model comparison and results analysis
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                    # Model hyperparameters and registry
â”‚   â”œâ”€â”€ data.py                      # UFCData class: manages data splits and transformations
â”‚   â”œâ”€â”€ helpers.py                   # Utility and preprocessing functions
â”‚   â”œâ”€â”€ io_model.py                  # Save/load model objects from disk
â”‚   â”œâ”€â”€ metrics.py                   # Evaluation metrics and plots
â”‚   â”œâ”€â”€ model.py                     # Wrapper class for saving, loading, and evaluating models
â”‚   â”œâ”€â”€ model_factory.py             # Central model selection logic
â”œâ”€â”€ docs/                            # Markdown documentation per model
â”œâ”€â”€ img/                             # Images for plots, logos, and visuals
â””â”€â”€ requirements.txt                 # Project dependencies


```

---

## ğŸš€ Getting Started

To run the pipeline locally:

1. **Clone the repository**

```bash
git clone https://github.com/mfourier/ufc-predictor.git
cd ufc-predictor
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
```

3. **Run the notebooks** Start from `notebooks/01-etl.ipynb` and proceed step by step through to `05-model_experiments.ipynb`.

---

## ğŸ“š Documentation

Comprehensive project documentation is available in the `docs/` folder, covering:

- **Model overviews and mathematical formulations**: Detailed descriptions of each algorithm, including underlying principles and expected behavior.
- **Key assumptions and limitations**: Insights into when and why each model performs best, as well as potential pitfalls.
- **Hyperparameter grids**: Full parameter configurations used for tuning with `GridSearchCV`, enabling reproducibility and extension.
- **Usage guides**: Step-by-step instructions on running the notebooks, customizing experiments, and interpreting results.

---

## ğŸ‘¥ Contributors

- **Maximiliano Lioi** â€” M.Sc. in Applied Mathematics @ University of Chile
- **RocÃ­o YÃ¡Ã±ez** â€” M.Sc. in Applied Mathematics @ University of Chile

---

## ğŸ™ Acknowledgements

We thank [shortlikeafox](https://github.com/shortlikeafox/ultimate_ufc_dataset) for their excellent work compiling the UFC dataset used as the foundation of this project. Their contribution made it possible to train and evaluate predictive models on historical fight outcomes.

---
